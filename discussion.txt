Precision measures the proportion of positively predicted classes or labels that the model predicts correctly. Precision is also known as the positive predictive value. Recall on the other hand measures how good a model is at identifying and labelling all actual positives out of all positives that exist within a given dataset. 

The Precision Score (How often my model makes the right prediction of Location) for wiki 2 is 50%. That is when my model predicts a location label for a text, the text is actually a location 50% of the time. The Recall score of 12.5% means that if a location is in the wiki 2 text data, my model can identify it 12.5% of the time. The evaluation metrics of wiki 2 also implies that if a person is in the text, the model can identify it 100% of the time. 

For wiki 3, when my model predicts a location label for a text, it is usually a location 100% of the time and the recall for a location label, I.e if a location is in the wiki 3 text data, my model can identify it 12.5% of the time. The recall for a person in my test data for wiki 3 is 20% and the precision that is how often the model makes the right prediction for a person is 1.72%. 

For wiki 1, the recall for an organisation label, I.e if an organisation  is in the wiki 1 text data, my model can identify it 100% of the time. 

The F1 score is simply a function of recall and precision score. The closer the F1 score is to 0. The better the model. The False Positives, True Positives and True Negative predicted Values are also given in the wiki[123]-eval.txt. 